{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJwnCyDZOo0x"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# df1=pd.read_csv(\"/content/kidnapping.csv\")\n",
        "# df2=pd.read_csv(\"/content/murder.csv\")\n",
        "# df3=pd.read_csv(\"/content/sexhar.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/final.csv\")"
      ],
      "metadata": {
        "id": "Ln5F_bOBm-D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "zvhAehahltTE",
        "outputId": "dea2036a-1bd3-4bcc-9ea5-b4c65331fc22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               text    label\n",
              "0  At about 8:00 pm on 10.09.1996, the\\r\\ndecease...  murder\n",
              "1  between 26th and 27th November, 2000 when all ...  murder\n",
              "2  The case of the prosecution in brief is that, ...  murder\n",
              "3  I submit that my wife burnt to death this\\r\\ne...  murder\n",
              "4  They entered the house and saw\\r\\nParamjit Sin...  murder"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2533f598-d05e-471d-bfc4-80d8eadb1af4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>At about 8:00 pm on 10.09.1996, the\\r\\ndecease...</td>\n",
              "      <td>murder</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>between 26th and 27th November, 2000 when all ...</td>\n",
              "      <td>murder</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The case of the prosecution in brief is that, ...</td>\n",
              "      <td>murder</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I submit that my wife burnt to death this\\r\\ne...</td>\n",
              "      <td>murder</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>They entered the house and saw\\r\\nParamjit Sin...</td>\n",
              "      <td>murder</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2533f598-d05e-471d-bfc4-80d8eadb1af4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2533f598-d05e-471d-bfc4-80d8eadb1af4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2533f598-d05e-471d-bfc4-80d8eadb1af4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.label.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBPvUTUctwBi",
        "outputId": "ef925d63-db71-4b5a-cf70-d638ece1c83a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count                152\n",
              "unique                 3\n",
              "top       sexual offense\n",
              "freq                  51\n",
              "Name: label, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnZxGAIfO2f2",
        "outputId": "7fe34384-b014-430b-b303-5c70f183ab77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     object\n",
              "label    object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#df['word_length'] = df['cleanData'].apply(lambda x:len(x.split()))\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['target']=df.label.map({\"murder\":0,\"kidnapping\":1,\"sexual offense\":2})"
      ],
      "metadata": {
        "id": "gwX6Eez0ofM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns=['text','label','target']"
      ],
      "metadata": {
        "id": "5Dv_y59HtUhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.target.astype(int)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLG-ELOBtWEa",
        "outputId": "9ca4d753-6ebb-4a28-97f5-26ea7e28c751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      0\n",
              "1      0\n",
              "2      0\n",
              "3      0\n",
              "4      0\n",
              "      ..\n",
              "147    1\n",
              "148    1\n",
              "149    1\n",
              "150    1\n",
              "151    1\n",
              "Name: target, Length: 152, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGZF26HYO4sv"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test, y_train,y_test = train_test_split(df.text,df.target, test_size = 0.2, stratify = df.target,random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJA4uEwLO78h"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "num_words = 10000 # this means 10000 unique words can be taken \n",
        "tokenizer=Tokenizer(num_words,lower=True)\n",
        "df_total = pd.concat([X_train, X_test], axis = 0)\n",
        "tokenizer.fit_on_texts(df_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1nqe8UdPAGI",
        "outputId": "383e8d94-8ad6-4e50-cf6b-c1b34a1851b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3447"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymcAMhb1PDv1"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X_train_ =tokenizer.texts_to_sequences(X_train)\n",
        "X_train_pad=pad_sequences(X_train_,maxlen=171,padding='post')\n",
        "X_test_ = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_pad = pad_sequences(X_test_, maxlen = 171, padding = 'post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QS3alF7XPE_W",
        "outputId": "849dda93-0ed2-421c-df4b-959017523242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(121, 171) (31, 171)\n"
          ]
        }
      ],
      "source": [
        "print(X_train_pad.shape,X_test_pad.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXgcK7FRPI5y"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "glove_gensim  = api.load('glove-wiki-gigaword-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ViLM4eAPXJe",
        "outputId": "7a127c26-90e9-4f02-b0cc-b55d1b464188"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "glove_gensim['cat'].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0BRkRJIPZve"
      },
      "outputs": [],
      "source": [
        "vector_size = 300\n",
        "gensim_weight_matrix = np.zeros((num_words ,vector_size))\n",
        "gensim_weight_matrix.shape\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index < num_words: # since index starts with zero \n",
        "        if word in glove_gensim.vocab:\n",
        "            gensim_weight_matrix[index] = glove_gensim[word]\n",
        "        else:\n",
        "            gensim_weight_matrix[index] = np.zeros(300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMh4ZzuTPcxf",
        "outputId": "fc1dfdae-cbf7-489e-f3af-77d7568a52d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 300)\n"
          ]
        }
      ],
      "source": [
        "print(gensim_weight_matrix.shape)\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional\n",
        "import tensorflow\n",
        "from keras.models import Sequential\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM,CuDNNGRU\n",
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq1M2FRNPdW0"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIM = 300 # this means the embedding layer will create  a vector in 100 dimension\n",
        "model_gensim = Sequential()\n",
        "model_gensim.add(Embedding(input_dim = num_words,# the whole vocabulary size \n",
        "                          output_dim = EMBEDDING_DIM, # vector space dimension\n",
        "                          input_length= X_train_pad.shape[1], # max_len of text sequence\n",
        "                          weights = [gensim_weight_matrix],trainable = False))\n",
        "model_gensim.add(Dropout(0.2))\n",
        "model_gensim.add(Bidirectional(CuDNNLSTM(300,return_sequences=True)))\n",
        "model_gensim.add(Dropout(0.2))\n",
        "model_gensim.add(Bidirectional(CuDNNLSTM(400,return_sequences=True)))\n",
        "model_gensim.add(Dropout(0.2))\n",
        "model_gensim.add(Bidirectional(CuDNNLSTM(400,return_sequences=True)))\n",
        "model_gensim.add(Dropout(0.2))\n",
        "model_gensim.add(Bidirectional(CuDNNLSTM(200,return_sequences=False)))\n",
        "model_gensim.add(Dense(3, activation = 'softmax'))\n",
        "model_gensim.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'adam',metrics = 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ijw43OVbPmVa",
        "outputId": "a08313e8-547d-45a7-c3d0-92c71d8f7d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 171, 300)          3000000   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 171, 300)          0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 171, 600)         1444800   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 171, 600)          0         \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 171, 800)         3206400   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 171, 800)          0         \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 171, 800)         3846400   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 171, 800)          0         \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 400)              1603200   \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 1203      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,102,003\n",
            "Trainable params: 10,102,003\n",
            "Non-trainable params: 3,000,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_gensim.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0iexeZePm2_"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "roc= EarlyStopping(monitor='auc_roc', patience=300, verbose=1, mode='max')\n",
        "es = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 8)\n",
        "mc = ModelCheckpoint('./model_gensim.h5', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ4BkGn_K9TU",
        "outputId": "f1687f56-f8d4-437e-ab53-785140a414c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.0912 - accuracy: 0.3802\n",
            "Epoch 1: val_accuracy improved from -inf to 0.35484, saving model to ./model_gensim.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 9s 2s/step - loss: 1.0912 - accuracy: 0.3802 - val_loss: 1.8167 - val_accuracy: 0.3548\n",
            "Epoch 2/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 1.6522 - accuracy: 0.4050\n",
            "Epoch 2: val_accuracy improved from 0.35484 to 0.51613, saving model to ./model_gensim.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 646ms/step - loss: 1.6522 - accuracy: 0.4050 - val_loss: 1.0567 - val_accuracy: 0.5161\n",
            "Epoch 3/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.9796 - accuracy: 0.5289\n",
            "Epoch 3: val_accuracy did not improve from 0.51613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 217ms/step - loss: 0.9796 - accuracy: 0.5289 - val_loss: 1.0578 - val_accuracy: 0.4194\n",
            "Epoch 4/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.8982 - accuracy: 0.5620\n",
            "Epoch 4: val_accuracy improved from 0.51613 to 0.67742, saving model to ./model_gensim.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 547ms/step - loss: 0.8982 - accuracy: 0.5620 - val_loss: 0.9420 - val_accuracy: 0.6774\n",
            "Epoch 5/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.7663 - accuracy: 0.6694\n",
            "Epoch 5: val_accuracy did not improve from 0.67742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 220ms/step - loss: 0.7663 - accuracy: 0.6694 - val_loss: 0.9885 - val_accuracy: 0.6452\n",
            "Epoch 6/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.6817 - accuracy: 0.6942\n",
            "Epoch 6: val_accuracy did not improve from 0.67742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 223ms/step - loss: 0.6817 - accuracy: 0.6942 - val_loss: 0.8136 - val_accuracy: 0.6774\n",
            "Epoch 7/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.5615 - accuracy: 0.7934\n",
            "Epoch 7: val_accuracy did not improve from 0.67742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 218ms/step - loss: 0.5615 - accuracy: 0.7934 - val_loss: 0.8202 - val_accuracy: 0.6452\n",
            "Epoch 8/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.4889 - accuracy: 0.7934\n",
            "Epoch 8: val_accuracy improved from 0.67742 to 0.74194, saving model to ./model_gensim.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 540ms/step - loss: 0.4889 - accuracy: 0.7934 - val_loss: 0.7595 - val_accuracy: 0.7419\n",
            "Epoch 9/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.4095 - accuracy: 0.8512\n",
            "Epoch 9: val_accuracy improved from 0.74194 to 0.80645, saving model to ./model_gensim.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 553ms/step - loss: 0.4095 - accuracy: 0.8512 - val_loss: 0.7407 - val_accuracy: 0.8065\n",
            "Epoch 10/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.3633 - accuracy: 0.9008\n",
            "Epoch 10: val_accuracy did not improve from 0.80645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 218ms/step - loss: 0.3633 - accuracy: 0.9008 - val_loss: 0.7562 - val_accuracy: 0.7742\n",
            "Epoch 11/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.9091\n",
            "Epoch 11: val_accuracy did not improve from 0.80645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 218ms/step - loss: 0.2808 - accuracy: 0.9091 - val_loss: 0.9050 - val_accuracy: 0.7097\n",
            "Epoch 12/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2339 - accuracy: 0.9256\n",
            "Epoch 12: val_accuracy did not improve from 0.80645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 219ms/step - loss: 0.2339 - accuracy: 0.9256 - val_loss: 0.9328 - val_accuracy: 0.6452\n",
            "Epoch 13/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9174\n",
            "Epoch 13: val_accuracy did not improve from 0.80645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 216ms/step - loss: 0.2176 - accuracy: 0.9174 - val_loss: 0.7277 - val_accuracy: 0.7742\n",
            "Epoch 14/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9669\n",
            "Epoch 14: val_accuracy did not improve from 0.80645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 221ms/step - loss: 0.1204 - accuracy: 0.9669 - val_loss: 0.9691 - val_accuracy: 0.7097\n",
            "Epoch 15/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9256\n",
            "Epoch 15: val_accuracy did not improve from 0.80645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 226ms/step - loss: 0.1731 - accuracy: 0.9256 - val_loss: 1.1341 - val_accuracy: 0.6774\n",
            "Epoch 16/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0938 - accuracy: 0.9669\n",
            "Epoch 16: val_accuracy did not improve from 0.80645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 218ms/step - loss: 0.0938 - accuracy: 0.9669 - val_loss: 1.2577 - val_accuracy: 0.6774\n",
            "Epoch 17/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.1001 - accuracy: 0.9587\n",
            "Epoch 17: val_accuracy did not improve from 0.80645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 225ms/step - loss: 0.1001 - accuracy: 0.9587 - val_loss: 1.1235 - val_accuracy: 0.7419\n",
            "Epoch 18/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0439 - accuracy: 0.9835\n",
            "Epoch 18: val_accuracy did not improve from 0.80645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 226ms/step - loss: 0.0439 - accuracy: 0.9835 - val_loss: 1.1522 - val_accuracy: 0.7419\n",
            "Epoch 19/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9669\n",
            "Epoch 19: val_accuracy did not improve from 0.80645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 222ms/step - loss: 0.0726 - accuracy: 0.9669 - val_loss: 0.8641 - val_accuracy: 0.7742\n",
            "Epoch 20/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9752\n",
            "Epoch 20: val_accuracy did not improve from 0.80645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 225ms/step - loss: 0.0735 - accuracy: 0.9752 - val_loss: 0.9106 - val_accuracy: 0.7419\n",
            "Epoch 21/60\n",
            "2/2 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9835\n",
            "Epoch 21: val_accuracy did not improve from 0.80645\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `auc_roc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2/2 [==============================] - 1s 221ms/step - loss: 0.0316 - accuracy: 0.9835 - val_loss: 0.8466 - val_accuracy: 0.7742\n",
            "Epoch 21: early stopping\n"
          ]
        }
      ],
      "source": [
        "history_gensim = model_gensim.fit(X_train_pad,y_train, epochs = 60, batch_size = 100, validation_data=(X_test_pad, y_test),verbose = 1, callbacks= [es, mc,roc]  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "672CTyRxP6kJ",
        "outputId": "1bda4a74-ada0-4509-9d78-5329c835b9fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 127ms/step - loss: 0.8466 - accuracy: 0.7742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8466134071350098, 0.774193525314331]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "model_gensim.evaluate(X_test_pad, y_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jhF8gaoP7XN"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "b02FMKC3PtAW",
        "outputId": "8402a61c-2abf-4af9-d5f2-194b62e06d76"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(history_gensim.history['accuracy'],c='b',label='train accuracy')\n",
        "plt.plot(history_gensim.history['val_accuracy'],c='r',label='validation accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF5-mWasG3N3",
        "outputId": "36e55885-5595-423f-d85f-3fb435b3432f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 127ms/step - loss: 0.8466 - accuracy: 0.7742\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8466134071350098, 0.774193525314331]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model_gensim.evaluate(X_test_pad, y_test) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84PnAP8uHlXx",
        "outputId": "7d50b8b2-177e-44c3-a828-e4a4b557b52e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 48ms/step\n",
            "[[0.00239872 0.00337697 0.99422437]] sexual offense\n"
          ]
        }
      ],
      "source": [
        "new_complaint = ['On 6.11.1995, Nasima Begum (PW.1), aged about 16 years filed a complaint alleging that on that day while she was going to attend her tuition alongwith her friend Nilufa Khatun, she met the appellant on the way who suddenly came and forcibly caught hold of her hair and planted a kiss, resultantly, she suffered a cut over her lower lip and started bleeding.']\n",
        "seq = tokenizer.texts_to_sequences(new_complaint)\n",
        "padded = pad_sequences(seq, maxlen=171,padding='post')\n",
        "pred = model_gensim.predict(padded)\n",
        "labels = ['murder','kidnapping','sexual offense']\n",
        "print(pred, labels[np.argmax(pred)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred=model_gensim.predict(X_test_pad)\n",
        "predicted = np.argmax(y_pred, axis=1)\n",
        "report = classification_report(y_test, predicted)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "VthLhl9rnd5z",
        "outputId": "d33ca43f-4d04-4f13-da7c-8ba82a0af0e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 40ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.70      0.70        10\n",
            "           1       0.89      0.73      0.80        11\n",
            "           2       0.75      0.90      0.82        10\n",
            "\n",
            "    accuracy                           0.77        31\n",
            "   macro avg       0.78      0.78      0.77        31\n",
            "weighted avg       0.78      0.77      0.77        31\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}